Finished preprocessing the CelebA dataset...
200600
torch.cuda.is_available(): True
torch.cuda.get_device_name(): NVIDIA RTX A6000
Finished preprocessing the CelebA dataset...
200600
Loading the trained models from step 30000...
Start training...
  0%|                                                                                                                                                                                                                                                | 0/150000 [00:00<?, ?it/s]/home/yerinyoon/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
  0%|                                                                                                                                                                                                                                     | 7/150000 [00:05<19:12:15,  2.17it/s]

  0%|                                                                                                                                                                                                                                     | 24/150000 [00:07<4:36:36,  9.04it/s]
Elapsed [0:00:06], Iteration [30020/180000], D/loss_real: -2.6726, D/loss_fake: 0.7009, D/loss_cls: 2.4712, D/loss_gp: 0.0313, G/loss_fake: -0.9872, G/loss_rec: 0.1500, G/loss_cls: 0.6080

  0%|                                                                                                                                                                                                                                     | 44/150000 [00:09<4:12:45,  9.89it/s]
Elapsed [0:00:09], Iteration [30040/180000], D/loss_real: -2.0058, D/loss_fake: 0.3380, D/loss_cls: 1.5524, D/loss_gp: 0.0076, G/loss_fake: -0.8740, G/loss_rec: 0.1429, G/loss_cls: 0.8919

  0%|                                                                                                                                                                                                                                     | 62/150000 [00:11<4:33:24,  9.14it/s]

  0%|                                                                                                                                                                                                                                     | 80/150000 [00:13<4:59:50,  8.33it/s]
Elapsed [0:00:12], Iteration [30070/180000], D/loss_real: -2.1304, D/loss_fake: 0.2939, D/loss_cls: 1.2707, D/loss_gp: 0.0108, G/loss_fake: -0.3139, G/loss_rec: 0.1230, G/loss_cls: 0.5634

  0%|▏                                                                                                                                                                                                                                    | 98/150000 [00:15<4:27:41,  9.33it/s]
Elapsed [0:00:14], Iteration [30090/180000], D/loss_real: -3.5898, D/loss_fake: 2.2229, D/loss_cls: 1.7787, D/loss_gp: 0.0114, G/loss_fake: -1.9059, G/loss_rec: 0.1117, G/loss_cls: 0.8214
  0%|▏                                                                                                                                                                                                                                   | 110/150000 [00:16<6:25:11,  6.49it/s]
Traceback (most recent call last):
  File "/home/yerinyoon/code/anonymousNet/starGAN/stargan.py", line 165, in <module>
    main(config)
  File "/home/yerinyoon/code/anonymousNet/starGAN/stargan.py", line 87, in main
    solver.train(config)
  File "/home/yerinyoon/code/anonymousNet/starGAN/model.py", line 453, in train
    d_loss_gp = self.gradient_penalty(out_src, x_hat)
  File "/home/yerinyoon/code/anonymousNet/starGAN/model.py", line 247, in gradient_penalty
    dydx = torch.autograd.grad(outputs=y,
  File "/home/yerinyoon/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py", line 300, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Elapsed [0:00:16], Iteration [30110/180000], D/loss_real: -3.5903, D/loss_fake: 1.8744, D/loss_cls: 1.5629, D/loss_gp: 0.0095, G/loss_fake: -2.2214, G/loss_rec: 0.1111, G/loss_cls: 0.6016